services:
  # Ollama - LLM Service
  ollama:
    image: ollama/ollama:0.14.3
    container_name: researcho-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      OLLAMA_HOST: 0.0.0.0:11434
    networks:
      - research-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 3
              capabilities: [gpu]

  caddy:
    container_name: research-caddy
    image: docker.io/library/caddy:2-alpine
    network_mode: host
    restart: unless-stopped
    volumes:
      - ./searxng/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy-data:/data:rw
      - caddy-config:/config:rw
    environment:
      - SEARXNG_HOSTNAME=${SEARXNG_HOSTNAME:-http://localhost}
      - SEARXNG_TLS=${LETSENCRYPT_EMAIL:-internal}
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  redis:
    container_name: research-redis
    image: docker.io/valkey/valkey:8-alpine
    command: valkey-server --save 30 1 --loglevel warning
    restart: unless-stopped
    networks:
      - research-network
    volumes:
      - valkey-data2:/data
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"


  searxng:
    container_name: research-searxng
    image: searxng/searxng:latest
    restart: unless-stopped
    networks:
      - research-network
    ports:
      - "127.0.0.1:8080:8080"
    volumes:
      - ./searxng/settings.yml:/etc/searxng/settings.yml:ro
      - searxng-data:/var/cache/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3


  # SearXNG - Meta Search Engine
  # searxng:
  #   image: searxng/searxng:latest
  #   container_name: deep-research-searxng
  #   ports:
  #     - "8080:8080"
  #   volumes:
  #     - ./searxng/settings.yml:/etc/searxng/settings.yml:ro
  #   environment:
  #     BIND_ADDRESS: 0.0.0.0:8080
  #   networks:
  #     - deep-research-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # Crawl4AI - Web Scraping Service
  crawl4ai:
    image: unclecode/crawl4ai:latest
    container_name: research-crawl4ai
    ports:
      - "11235:11235"
    environment:
      API_PORT: 11235
      LOG_LEVEL: INFO
    networks:
      - research-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11235/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Lightning Server - Agent Orchestration with APO & VERL
  lightning-server:
    image: lightning-server:cuda
    #build:
    #  context: ./localhost
    #  dockerfile: Dockerfile
    #  target: cuda
    container_name: research-lightning-server
    ports:
      - "9090:9090"
    environment:
      LIGHTNING_PORT: 9090
      APO_ENABLED: "true"
      VERL_ENABLED: "true"
      APO_STRATEGY: "balanced"
      APO_MAX_TASKS: 10
      APO_TASK_TIMEOUT: 300
      VERL_TRACKING_ENABLED: "true"
      VERL_CONFIDENCE_THRESHOLD: 0.7
      LOG_LEVEL: INFO
    networks:
      - research-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Deep Research Agent - Main Orchestrator
  # research-agent:
  #   build:
  #     context: ./DeepResearchAgent
  #     dockerfile: Dockerfile
  #   container_name: deep-research-agent
  #   ports:
  #     - "5000:5000"
  #   environment:
  #     LIGHTNING_SERVER_URL: http://localhost:9090
  #     OLLAMA_ENDPOINT: http://ollama:11434
  #     SEARXNG_ENDPOINT: http://searxng:8080
  #     CRAWL4AI_ENDPOINT: http://crawl4ai:11235
  #   networks:
  #     - deep-research-network
  #   depends_on:
  #     ollama:
  #       condition: service_healthy
  #     searxng:
  #       condition: service_healthy
  #     crawl4ai:
  #       condition: service_healthy
  #     lightning-server:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # Qdrant - Vector Database for Semantic Search
  qdrant:
    image: qdrant/qdrant:latest
    container_name: research-qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      QDRANT_API_KEY: ${QDRANT_API_KEY:-}
      LOG_LEVEL: info
    networks:
      - research-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  # searxng:
  deep-research-network:
    driver: bridge

volumes:
  caddy-data:
  caddy-config:
  valkey-data2:
  searxng-data:
  ollama_data:
    driver: local
  qdrant_storage:
    driver: local
